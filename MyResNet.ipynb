{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Flatten, Dense, Dropout, Reshape, Permute, Activation, BatchNormalization, \\\n",
    "    Input\n",
    "from keras.engine.topology import merge\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "import numpy as np\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from image_util import crop_center\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "Xtrain = np.load('./data/dataset256/Xtrain.npy')\n",
    "Xtest = np.load('./data/dataset256/Xtest.npy')\n",
    "Ytrain_reg = np.load('./data/dataset256/Ytrain_reg.npy')\n",
    "Ytest_reg = np.load('./data/dataset256/Ytest_reg.npy')\n",
    "Ytrain_cls = np.load('./data/dataset256/Ytrain_cls.npy')\n",
    "Ytest_cls = np.load('./data/dataset256/Ytrain_cls.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# netowrk settings\n",
    "\n",
    "batch_size = 64\n",
    "nb_epoch = 2\n",
    "folder = './myresnet-cls-training-checkpoint/'\n",
    "load_path = False\n",
    "n_cls = int(np.max(Ytrain_cls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ytrain_cls = to_categorical(Ytrain_cls-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def addResidualBlock(inp, size=(3,3), subsample=(4,4), filters_conv_rate=2, name=None):\n",
    "    conv_filters = int(inp.get_shape()[-1])*filters_conv_rate\n",
    "    inp = BatchNormalization()(inp)\n",
    "    first = Convolution2D(conv_filters, size[0], size[1], border_mode='same', subsample=subsample, activation='relu')(inp)\n",
    "    \n",
    "    second = BatchNormalization()(first)\n",
    "    second = Convolution2D(conv_filters, 3,3, border_mode='same', activation='relu')(second)\n",
    "    \n",
    "    second = BatchNormalization()(second)\n",
    "    second = Convolution2D(conv_filters, 3,3, border_mode='same', activation='relu')(second)\n",
    "    \n",
    "    return merge([first, second], mode='sum', name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MyResNetRegr():\n",
    "    inp = Input(shape=(256, 256,3))\n",
    "    out = Convolution2D(16, 3,3, border_mode='same', activation='relu', subsample=(4,4))(inp)\n",
    "    for i in range(3):\n",
    "        out = addResidualBlock(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = Activation('relu')(out)\n",
    "    \n",
    "    out = BatchNormalization()(out)\n",
    "    out = Flatten()(out)\n",
    "    out = Dense(128, activation='relu', name='fcc')(out)\n",
    "    out = Dense(1, name='out')(out)\n",
    "    \n",
    "    model = Model(input=inp, output=out)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def MyResNetCls():\n",
    "    inp = Input(shape=(256, 256,3))\n",
    "    out = Convolution2D(16, 3,3, border_mode='same', activation='relu', subsample=(4,4))(inp)\n",
    "    for i in range(3):\n",
    "        out = addResidualBlock(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = Activation('relu')(out)\n",
    "    \n",
    "    out = BatchNormalization()(out)\n",
    "    out = Flatten()(out)\n",
    "    out = Dense(128, activation='relu', name='fcc')(out)\n",
    "    out = Dense(n_cls, activation='softmax', name='out')(out)\n",
    "    \n",
    "    model = Model(input=inp, output=out)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 256, 256, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_21 (Convolution2D) (None, 64, 64, 16)    448         input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_27 (BatchNorm (None, 64, 64, 16)    64          convolution2d_21[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_22 (Convolution2D) (None, 16, 16, 32)    4640        batchnormalization_27[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_28 (BatchNorm (None, 16, 16, 32)    128         convolution2d_22[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_23 (Convolution2D) (None, 16, 16, 32)    9248        batchnormalization_28[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_29 (BatchNorm (None, 16, 16, 32)    128         convolution2d_23[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_24 (Convolution2D) (None, 16, 16, 32)    9248        batchnormalization_29[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "merge_7 (Merge)                  (None, 16, 16, 32)    0           convolution2d_22[0][0]           \n",
      "                                                                   convolution2d_24[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_30 (BatchNorm (None, 16, 16, 32)    128         merge_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 16, 16, 32)    0           batchnormalization_30[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_31 (BatchNorm (None, 16, 16, 32)    128         activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_25 (Convolution2D) (None, 4, 4, 64)      18496       batchnormalization_31[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_32 (BatchNorm (None, 4, 4, 64)      256         convolution2d_25[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_26 (Convolution2D) (None, 4, 4, 64)      36928       batchnormalization_32[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_33 (BatchNorm (None, 4, 4, 64)      256         convolution2d_26[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_27 (Convolution2D) (None, 4, 4, 64)      36928       batchnormalization_33[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "merge_8 (Merge)                  (None, 4, 4, 64)      0           convolution2d_25[0][0]           \n",
      "                                                                   convolution2d_27[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_34 (BatchNorm (None, 4, 4, 64)      256         merge_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 4, 4, 64)      0           batchnormalization_34[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_35 (BatchNorm (None, 4, 4, 64)      256         activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_28 (Convolution2D) (None, 1, 1, 128)     73856       batchnormalization_35[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_36 (BatchNorm (None, 1, 1, 128)     512         convolution2d_28[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_29 (Convolution2D) (None, 1, 1, 128)     147584      batchnormalization_36[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_37 (BatchNorm (None, 1, 1, 128)     512         convolution2d_29[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_30 (Convolution2D) (None, 1, 1, 128)     147584      batchnormalization_37[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "merge_9 (Merge)                  (None, 1, 1, 128)     0           convolution2d_28[0][0]           \n",
      "                                                                   convolution2d_30[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_38 (BatchNorm (None, 1, 1, 128)     512         merge_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 1, 1, 128)     0           batchnormalization_38[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_39 (BatchNorm (None, 1, 1, 128)     512         activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 128)           0           batchnormalization_39[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "fcc (Dense)                      (None, 128)           16512       flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "out (Dense)                      (None, 7)             903         fcc[0][0]                        \n",
      "====================================================================================================\n",
      "Total params: 506,023\n",
      "Trainable params: 504,199\n",
      "Non-trainable params: 1,824\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if load_path:\n",
    "    model = load_model(load_path)\n",
    "else:\n",
    "    model = MyResNetCls()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# batch logger \n",
    "# ref: https://github.com/fchollet/keras/issues/2850#issuecomment-222542429\n",
    "class NBatchLogger(Callback):\n",
    "    def __init__(self,display=100):\n",
    "        '''\n",
    "        display: Number of batches to wait before outputting loss\n",
    "        '''\n",
    "        self.seen = 0\n",
    "        self.display = display\n",
    "\n",
    "    def on_batch_end(self,batch,logs={}):\n",
    "        self.seen += logs.get('size', 0)\n",
    "        if self.seen % self.display == 0:\n",
    "            print('\\n{0}/{1} - Batch Loss: {2}'.format(self.seen,self.params['nb_sample'],\n",
    "                                                self.params['metrics'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/2\n",
      "\n",
      "64/900 - Batch Loss: loss\n",
      " 64/900 [=>............................] - ETA: 84s - loss: 2.1175\n",
      "128/900 - Batch Loss: loss\n",
      "128/900 [===>..........................] - ETA: 42s - loss: 2.0771\n",
      "192/900 - Batch Loss: loss\n",
      "192/900 [=====>........................] - ETA: 28s - loss: 2.0610\n",
      "256/900 - Batch Loss: loss\n",
      "256/900 [=======>......................] - ETA: 20s - loss: 2.0391\n",
      "320/900 - Batch Loss: loss\n",
      "320/900 [=========>....................] - ETA: 16s - loss: 2.0312\n",
      "384/900 - Batch Loss: loss\n",
      "384/900 [===========>..................] - ETA: 12s - loss: 1.9975\n",
      "448/900 - Batch Loss: loss\n",
      "448/900 [=============>................] - ETA: 10s - loss: 1.9586\n",
      "512/900 - Batch Loss: loss\n",
      "512/900 [================>.............] - ETA: 8s - loss: 1.9602 \n",
      "576/900 - Batch Loss: loss\n",
      "576/900 [==================>...........] - ETA: 6s - loss: 1.9496\n",
      "640/900 - Batch Loss: loss\n",
      "640/900 [====================>.........] - ETA: 4s - loss: 1.9493\n",
      "704/900 - Batch Loss: loss\n",
      "704/900 [======================>.......] - ETA: 3s - loss: 1.9248\n",
      "768/900 - Batch Loss: loss\n",
      "768/900 [========================>.....] - ETA: 2s - loss: 1.9093\n",
      "832/900 - Batch Loss: loss\n",
      "832/900 [==========================>...] - ETA: 1s - loss: 1.8934\n",
      "896/900 - Batch Loss: loss\n",
      "896/900 [============================>.] - ETA: 0s - loss: 1.8853\n",
      "900/900 - Batch Loss: loss\n",
      "900/900 [==============================] - 16s - loss: 1.8860 - val_loss: 7.1434\n",
      "Epoch 2/2\n",
      "\n",
      "964/900 - Batch Loss: loss\n",
      " 64/900 [=>............................] - ETA: 8s - loss: 1.4647\n",
      "1028/900 - Batch Loss: loss\n",
      "128/900 [===>..........................] - ETA: 7s - loss: 1.4430\n",
      "1092/900 - Batch Loss: loss\n",
      "192/900 [=====>........................] - ETA: 6s - loss: 1.4836\n",
      "1156/900 - Batch Loss: loss\n",
      "256/900 [=======>......................] - ETA: 6s - loss: 1.4794\n",
      "1220/900 - Batch Loss: loss\n",
      "320/900 [=========>....................] - ETA: 5s - loss: 1.4735\n",
      "1284/900 - Batch Loss: loss\n",
      "384/900 [===========>..................] - ETA: 5s - loss: 1.4470\n",
      "1348/900 - Batch Loss: loss\n",
      "448/900 [=============>................] - ETA: 4s - loss: 1.4697\n",
      "1412/900 - Batch Loss: loss\n",
      "512/900 [================>.............] - ETA: 3s - loss: 1.4692\n",
      "1476/900 - Batch Loss: loss\n",
      "576/900 [==================>...........] - ETA: 3s - loss: 1.4653\n",
      "1540/900 - Batch Loss: loss\n",
      "640/900 [====================>.........] - ETA: 2s - loss: 1.4714\n",
      "1604/900 - Batch Loss: loss\n",
      "704/900 [======================>.......] - ETA: 2s - loss: 1.4773\n",
      "1668/900 - Batch Loss: loss\n",
      "768/900 [========================>.....] - ETA: 1s - loss: 1.4749\n",
      "1732/900 - Batch Loss: loss\n",
      "832/900 [==========================>...] - ETA: 0s - loss: 1.4824\n",
      "1796/900 - Batch Loss: loss\n",
      "896/900 [============================>.] - ETA: 0s - loss: 1.4791\n",
      "1800/900 - Batch Loss: loss\n",
      "900/900 [==============================] - 10s - loss: 1.4793 - val_loss: 6.4107\n"
     ]
    }
   ],
   "source": [
    "#train just 1000 samples, 3 epochs\n",
    "# if False:\n",
    "checkpoint = ModelCheckpoint(filepath=folder + 'checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "out_batch = NBatchLogger(display=1)\n",
    "history = model.fit(x=Xtrain[:1000], y=Ytrain_cls[:1000], \n",
    "          nb_epoch=nb_epoch, batch_size=batch_size,validation_split=0.1, verbose=1, callbacks=[checkpoint]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-276f2479a8b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validation keras fix \n",
    "# ref: http://stackoverflow.com/questions/41796618/python-keras-cross-val-score-error/41841066#41841066\n",
    "from keras.wrappers.scikit_learn import BaseWrapper\n",
    "import copy\n",
    "\n",
    "def custom_get_params(self, **params):\n",
    "    res = copy.deepcopy(self.sk_params)\n",
    "    res.update({'build_fn': self.build_fn})\n",
    "    return res\n",
    "\n",
    "BaseWrapper.get_params = custom_get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3840/3852 [============================>.] - ETA: 0s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    }
   ],
   "source": [
    "evl = model.evaluate(x=Xtest, y=Ytest,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0042826834603561896"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002468179222131061"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regression \n",
    "# rev: http://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=MyNet, nb_epoch=nb_epoch, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, Xtrain[:1000], Ytrain[:1000], cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
